# Mini Data Warehouse â€“ TP1 : Simulation de ventes

## ğŸ¯ Objectifs pÃ©dagogiques

Ce projet a pour but de construire un mini Data Warehouse orientÃ© ventes, en simulant un pipeline complet depuis des donnÃ©es brutes jusquâ€™Ã  un schÃ©ma en Ã©toile exploitable pour lâ€™analyse.  
Les Ã©tapes clÃ©s sont :

- GÃ©nÃ©rer des donnÃ©es RAW (clients, produits, commandes)
- Nettoyer et normaliser les donnÃ©es (staging)
- Construire les dimensions : date, produit, client
- Construire la table de faits `fact_sales` avec KPIs (montant, coÃ»t, marge)
- RÃ©aliser des agrÃ©gations et visualiser le chiffre dâ€™affaires mensuel

---

## ğŸ—ï¸ Structure du projet

mini_dw_tp1/ â”œâ”€â”€ dw_example/ # Dossier de sortie des fichiers CSV â”‚ â”œâ”€â”€ raw_customers.csv â”‚ â”œâ”€â”€ raw_products.csv â”‚ â”œâ”€â”€ raw_orders.csv â”‚ â”œâ”€â”€ dim_date.csv â”‚ â”œâ”€â”€ dim_product.csv â”‚ â”œâ”€â”€ dim_customer.csv â”‚ â”œâ”€â”€ fact_sales.csv â”‚ â””â”€â”€ monthly_sales.png â”œâ”€â”€ src/ â”‚ â””â”€â”€ etl/ â”‚ â”œâ”€â”€ generate_raw.py â”‚ â”œâ”€â”€ staging.py â”‚ â”œâ”€â”€ build_dimensions.py â”‚ â”œâ”€â”€ build_fact_sales.py â”œâ”€â”€ notebooks/ â”‚ â”œâ”€â”€ exploration.ipynb â”‚ â””â”€â”€ visualisation.ipynb â”œâ”€â”€ run_all.py â”œâ”€â”€ requirements.txt â””â”€â”€ README.md


---

## âš™ï¸ Ã‰tapes du pipeline

### 1. GÃ©nÃ©ration des donnÃ©es RAW
- 120 clients, 60 produits, 2500 lignes de commande
- Export CSV : `raw_customers.csv`, `raw_products.csv`, `raw_orders.csv`

### 2. Staging (nettoyage)
- Standardisation des emails, pays, catÃ©gories, marques
- Conversion des dates
- PrÃ©paration des donnÃ©es pour les dimensions

### 3. Dimensions
- `dim_date` : extraite des dates de commande
- `dim_product` : produits uniques avec SK
- `dim_customer` : clients uniques avec SK

### 4. Table de faits `fact_sales`
- Jointures avec les dimensions
- Calculs : montant = quantitÃ© Ã— prix, coÃ»t, marge
- Colonnes : `order_id`, `order_line_id`, `date_sk`, `product_sk`, `customer_sk`, `quantity`, `unit_price`, `amount`, `cost`, `margin`

---

## ğŸ“Š Analyses et visualisations

- **Chiffre dâ€™affaires mensuel** : agrÃ©gation par mois et annÃ©e
- **Top 10 produits** : classement par chiffre dâ€™affaires
- **Graphique** : `monthly_sales.png` gÃ©nÃ©rÃ© avec matplotlib

---

## â–¶ï¸ ExÃ©cution du pipeline

1. Installer les dÃ©pendances :
   ```bash
   pip install -r requirements.txt
2. Lancer le pipeline complet
   python run_all.py
3. Ouvrir le notebook pour explorer les rÃ©sultats
    ```bash
   jupyter notebook

## ğŸ§  Concepts mobilisÃ©s

- Simulation rÃ©aliste de donnÃ©es transactionnelles  
- Nettoyage et enrichissement avec pandas  
- ModÃ©lisation en Ã©toile (dimensions + fait)  
- Calculs de KPIs : montant, coÃ»t, marge  
- Visualisation avec matplotlib  



# data-warehouse-
# ğŸ§¾ Mini Data Warehouse TP2



## ğŸ¯ Objectif du projet

Ce TP vise Ã  construire un mini Data Warehouse simulÃ©, en suivant toutes les Ã©tapes dâ€™un pipeline ETL : gÃ©nÃ©ration de donnÃ©es, nettoyage, modÃ©lisation en Ã©toile, automatisation, visualisation et requÃªtage SQL.  
Lâ€™objectif est de comprendre les fondements dâ€™un entrepÃ´t de donnÃ©es et de manipuler des outils concrets comme pandas, DuckDB et matplotlib.


## ğŸ—ï¸ Architecture du projet

Le projet est structurÃ© en plusieurs dossiers :

- `data/raw` : donnÃ©es simulÃ©es (production, pannes)
- `data/staging` : donnÃ©es nettoyÃ©es et enrichies
- `data/dim` : dimensions (date, machine, produit)
- `data/fact` : faits (production, downtime)
- `src/etl` : scripts Python pour chaque Ã©tape
- `notebooks` : exploration, visualisation, requÃªtes SQL


## âš™ï¸ Pipeline ETL

### a. GÃ©nÃ©ration des donnÃ©es (`generate_raw.py`)
- Simulation de 90 jours de production pour 3 machines et 3 produits
- DonnÃ©es de production et de pannes gÃ©nÃ©rÃ©es alÃ©atoirement

### b. Nettoyage et enrichissement (`staging.py`)
- Calcul des unitÃ©s bonnes, performance thÃ©orique, disponibilitÃ©
- AgrÃ©gation des pannes par jour et machine

### c. ModÃ©lisation en Ã©toile (`build_dimensions.py`)
- CrÃ©ation des dimensions avec clÃ©s substitutives
- Enrichissement de la dimension date (annÃ©e, mois, jour, jour de semaine)

### d. Construction des faits (`build_facts.py`)
- Jointures avec les dimensions
- CrÃ©ation des tables de faits prÃªtes pour lâ€™analyse

### e. Orchestration (`run_all.py`)
- ExÃ©cution automatique de toutes les Ã©tapes du pipeline


## ğŸ“Š Visualisation des indicateurs

### a. OEE mensuel par machine (`visualisation.ipynb`)
- Calcul de lâ€™OEE : disponibilitÃ© Ã— performance Ã— qualitÃ©
- TracÃ© de lâ€™Ã©volution mensuelle par machine

### b. Exploration des donnÃ©es (`traitements.ipynb`)
- VÃ©rification des donnÃ©es Ã  chaque Ã©tape
- Statistiques descriptives et contrÃ´les qualitÃ©


## ğŸ§  RequÃªtes SQL analytiques

### a. Top produits les plus performants (`requetes_duckdb.ipynb`)
- RequÃªte SQL pour identifier les produits avec la meilleure performance moyenne

### b. RequÃªte OEE par machine et mois
- AgrÃ©gation SQL pour reproduire la visualisation en mode analytique

### c. RequÃªte personnalisÃ©e (optionnelle)
- PossibilitÃ© dâ€™ajouter des requÃªtes pour dÃ©tecter les machines en sous-performance ou analyser les pannes


## ğŸ“Œ RÃ©sultats et observations

- Les machines ont des performances variables selon les mois
- Le produit P1 est le plus performant dans cette simulation
- Le pipeline est entiÃ¨rement automatisÃ© et reproductible
- DuckDB permet des requÃªtes SQL rapides directement sur les fichiers CSV


## ğŸ§ª Limites et extensions possibles

- Simulation simple : pas de saisonnalitÃ© ni de logique mÃ©tier
- PossibilitÃ© dâ€™ajouter des dimensions supplÃ©mentaires (opÃ©rateurs, lignes de production)
- IntÃ©gration future avec une base SQL ou un outil BI


## ğŸ‘©â€ğŸ’» Conclusion

Ce projet mâ€™a permis de consolider mes compÃ©tences en :

- Structuration de projet analytique
- Manipulation de donnÃ©es avec pandas
- ModÃ©lisation en Ã©toile
- Automatisation avec Python
- Visualisation et requÃªtage SQL

Il constitue une base solide pour aborder des projets industriels plus complexes en Big Data et Intelligence Artificielle.

## âœ¨ Auteur

Projet rÃ©alisÃ© par **Helene Cakposse**  
*MsC 2 DATA ENGINEERING*  
ECE Paris

```python

```
