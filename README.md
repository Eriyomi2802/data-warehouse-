# data-warehouse-
# ğŸ§¾ Rapport Final â€” Mini Data Warehouse TP2

## RÃ©alisÃ© par : CAKPOSSE Eriyomi PhestÃ¨ce Chancelle-HÃ©lÃ¨ne


## ğŸ¯ Objectif du projet

Ce TP vise Ã  construire un mini Data Warehouse simulÃ©, en suivant toutes les Ã©tapes dâ€™un pipeline ETL : gÃ©nÃ©ration de donnÃ©es, nettoyage, modÃ©lisation en Ã©toile, automatisation, visualisation et requÃªtage SQL.  
Lâ€™objectif est de comprendre les fondements dâ€™un entrepÃ´t de donnÃ©es et de manipuler des outils concrets comme pandas, DuckDB et matplotlib.


## ğŸ—ï¸ Architecture du projet

Le projet est structurÃ© en plusieurs dossiers :

- `data/raw` : donnÃ©es simulÃ©es (production, pannes)
- `data/staging` : donnÃ©es nettoyÃ©es et enrichies
- `data/dim` : dimensions (date, machine, produit)
- `data/fact` : faits (production, downtime)
- `src/etl` : scripts Python pour chaque Ã©tape
- `notebooks` : exploration, visualisation, requÃªtes SQL


## âš™ï¸ Pipeline ETL

### a. GÃ©nÃ©ration des donnÃ©es (`generate_raw.py`)
- Simulation de 90 jours de production pour 3 machines et 3 produits
- DonnÃ©es de production et de pannes gÃ©nÃ©rÃ©es alÃ©atoirement

### b. Nettoyage et enrichissement (`staging.py`)
- Calcul des unitÃ©s bonnes, performance thÃ©orique, disponibilitÃ©
- AgrÃ©gation des pannes par jour et machine

### c. ModÃ©lisation en Ã©toile (`build_dimensions.py`)
- CrÃ©ation des dimensions avec clÃ©s substitutives
- Enrichissement de la dimension date (annÃ©e, mois, jour, jour de semaine)

### d. Construction des faits (`build_facts.py`)
- Jointures avec les dimensions
- CrÃ©ation des tables de faits prÃªtes pour lâ€™analyse

### e. Orchestration (`run_all.py`)
- ExÃ©cution automatique de toutes les Ã©tapes du pipeline


## ğŸ“Š Visualisation des indicateurs

### a. OEE mensuel par machine (`visualisation.ipynb`)
- Calcul de lâ€™OEE : disponibilitÃ© Ã— performance Ã— qualitÃ©
- TracÃ© de lâ€™Ã©volution mensuelle par machine

### b. Exploration des donnÃ©es (`traitements.ipynb`)
- VÃ©rification des donnÃ©es Ã  chaque Ã©tape
- Statistiques descriptives et contrÃ´les qualitÃ©


## ğŸ§  RequÃªtes SQL analytiques

### a. Top produits les plus performants (`requetes_duckdb.ipynb`)
- RequÃªte SQL pour identifier les produits avec la meilleure performance moyenne

### b. RequÃªte OEE par machine et mois
- AgrÃ©gation SQL pour reproduire la visualisation en mode analytique

### c. RequÃªte personnalisÃ©e (optionnelle)
- PossibilitÃ© dâ€™ajouter des requÃªtes pour dÃ©tecter les machines en sous-performance ou analyser les pannes


## ğŸ“Œ RÃ©sultats et observations

- Les machines ont des performances variables selon les mois
- Le produit P1 est le plus performant dans cette simulation
- Le pipeline est entiÃ¨rement automatisÃ© et reproductible
- DuckDB permet des requÃªtes SQL rapides directement sur les fichiers CSV


## ğŸ§ª Limites et extensions possibles

- Simulation simple : pas de saisonnalitÃ© ni de logique mÃ©tier
- PossibilitÃ© dâ€™ajouter des dimensions supplÃ©mentaires (opÃ©rateurs, lignes de production)
- IntÃ©gration future avec une base SQL ou un outil BI


## ğŸ‘©â€ğŸ’» Conclusion

Ce projet mâ€™a permis de consolider mes compÃ©tences en :

- Structuration de projet analytique
- Manipulation de donnÃ©es avec pandas
- ModÃ©lisation en Ã©toile
- Automatisation avec Python
- Visualisation et requÃªtage SQL

Il constitue une base solide pour aborder des projets industriels plus complexes en Big Data et Intelligence Artificielle.



```python

```
